# This values.yaml file creates the resources for Qwen/Qwen3-30B-A3B-FP8

# When true, LeaderWorkerSet is used instead of Deployment
multinode: true
# When true, an InferencePool is created.
# When false, an InferencePool is not created. Nor is the referencing HTTPRoute.
# Note that when there is no endpointPicker, the InferencePool is not created (overriding a true here).
inferencePool: true
# When true, an HTTPRoute is created.
# When false, an HTTPRoute is not created.
# Note that when inferencePool is false, or there is is no endpointPicker, the HTTPRoute is not created (overriding a true here).
httpRoute: true

# To control creation of the endpoint picker (inference scheduler), see endpointPicker below

routing:
  # This is the model name for the OpenAI request
  modelName: Qwen/Qwen3-30B-A3B-FP8
  servicePort: 8080   # Sidecar listens on this port for requests. If there's no sidecar, the request goes here
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
    targetPort: 8200
    debugLevel: 5
  parentRefs:
  - name: inference-gateway

modelArtifacts:
  # When specfying the URI with `hf` prefix, the <repo-id>/<model-id> string
  # is extracted and exposed as a template variable that can be used as {{ .HFModelName }}
  
  uri: "pvc://tms-hf-cache/model-cache"
  authSecretName: "hf-secret"
  size: 5Mi 

# describe decode pods
decode:
  autoscaling: 
    enabled: false
  replicas: 1

  parallelism:  
    data: 4
    tensor: 1
    dataLocal: 2
  
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200

  volumes:
    # Volume for the init script from ConfigMap
    - name: init-scripts-volume
      configMap:
        defaultMode: 0755
        name: vllm-init-scripts-config
    # Needed for NCCL to function
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 1Gi
    # - name: hf-cache
    #   persistentVolumeClaim:
    #     claimName: tms-hf-cache
    - name: vllm
      persistentVolumeClaim:
        claimName: tms-vllm

  containers:
    - name: vllm-worker
      image: "quay.io/tms/vllm-dev-pplx:0.1.0"
      imagePullPolicy: Always
      workingDir: /code
      stdin: true
      tty: true
      modelCommand: custom
      command: ["/bin/sh","-c"]
      args:
        - |
          #################
          # Interactive section
          #################
          # export VENV_PATH="/code/venv"
          # source /init-scripts/common.sh
          # /init-scripts/dotfiles.sh
          # sleep infinity
          #################
          #
          #################
          # Install vLLM
          #################
          VLLM_USE_PRECOMPILED=1 /init-scripts/vllm.sh
          #################
          # RUN vLLM
          #################
          START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
          if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
            #################
            # Leader-only launch
            #################
            exec /app/venv/bin/vllm serve \
              Qwen/Qwen3-30B-A3B-FP8 \
              --port 8200 \
              --disable-log-requests \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $DP_SIZE \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address $(LWS_LEADER_ADDRESS) \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --trust-remote-code
          else
            #################
            # Worker-only launch
            #################
            exec /app/venv/bin/vllm serve \
              Qwen/Qwen3-30B-A3B-FP8 \
              --port 8200 \
              --disable-log-requests \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $DP_SIZE \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address $(LWS_LEADER_ADDRESS) \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --trust-remote-code \
              --headless
          fi
##################
# Paste these args into the above vllm serve commands
# to enable EPLB and API server scaleout.
##################
#  --enable-eplb \
#  --num-redundant-experts 32 \
#  --eplb-window-size 1000 \
#  --eplb-step-interval 3000 \
##################
#  --api-server-count=8 \
##################
      env:
        - name: VLLM_TORCH_PROFILER_DIR
          value: "/code/traces"
        - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
          value: "1"
        # repo and branch are development versions
        # will want to use release version
        - name: VLLM_REPO_URL
          value: "https://github.com/tlrmchlsmth/vllm.git"
        - name: VLLM_BRANCH
          value: "for_michael_k"
        - name: VLLM_USE_DEEP_GEMM
          value: "1"
        - name: VLLM_ALL2ALL_BACKEND
        #  value: "naive"
          value: "pplx"
        #  value: "deepep_high_throughput"
        #  value: "deepep_low_latency"
        # Needed for GDRCOPY to be used.
        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        - name: NVSHMEM_DEBUG
          value: "INFO"
        # Uncomment for debugging
        #- name: NVSHMEM_DEBUG_SUBSYS
        #  value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "DEBUG"
        #MK - name: HF_HUB_CACHE
        #MK   value: /huggingface-cache
        - name: GH_TOKEN_FROM_SECRET
          valueFrom:
            secretKeyRef:
              name: gh-token-secret
              key: GH_TOKEN
              optional: true
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "6555"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP

      securityContext:
        capabilities:
          add:
          - "IPC_LOCK"
          - "SYS_RAWIO"
      resources:
        limits:
          nvidia.com/gpu: 1
          memory: 512Gi
          ephemeral-storage: 64Gi
          rdma/ib: 1
        requests:
          cpu: 32
          memory: 512Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: 1
          rdma/ib: 1
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - name: init-scripts-volume
          mountPath: /init-scripts
        #MK - name: hf-cache
        #MK   mountPath: /huggingface-cache
        - name: vllm
          mountPath: /code
      mountModelVolume: true

# describe prefill pods
prefill:
  autoscaling: 
    enabled: false
  replicas: 1

  parallelism:  
    data: 4
    tensor: 1
    dataLocal: 2
  
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200

  volumes:
    # Volume for the init script from ConfigMap
    - name: init-scripts-volume
      configMap:
        defaultMode: 0755
        name: vllm-init-scripts-config
    # Needed for NCCL to function
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 1Gi
    # - name: hf-cache
    #   persistentVolumeClaim:
    #     claimName: tms-hf-cache
    - name: vllm
      persistentVolumeClaim:
        claimName: tms-vllm


  containers:
    - name: vllm-worker
      image: "quay.io/tms/vllm-dev-pplx:0.1.0"
      imagePullPolicy: Always
      workingDir: /code
      stdin: true
      tty: true
      modelCommand: custom
      command: ["/bin/sh","-c"]
      args:
        - |
          #################
          # Interactive section
          #################
          # export VENV_PATH="/code/venv"
          # source /init-scripts/common.sh
          # /init-scripts/dotfiles.sh
          # sleep infinity
          #################
          #
          #################
          # Install vLLM
          #################
          VLLM_USE_PRECOMPILED=1 /init-scripts/vllm.sh
          #################
          # RUN vLLM
          #################
          START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
          if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
            #################
            # Leader-only launch
            #################
            exec /app/venv/bin/vllm serve \
              Qwen/Qwen3-30B-A3B-FP8 \
              --port 8080 \
              --disable-log-requests \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $DP_SIZE \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address $(LWS_LEADER_ADDRESS) \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --trust-remote-code
          else
            #################
            # Worker-only launch
            #################
            exec /app/venv/bin/vllm serve \
              Qwen/Qwen3-30B-A3B-FP8 \
              --port 8080 \
              --disable-log-requests \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $DP_SIZE \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address $(LWS_LEADER_ADDRESS) \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --trust-remote-code \
              --headless
          fi
##################
# Paste these args into the above vllm serve commands
# to enable EPLB and API server scaleout.
##################
#  --enable-eplb \
#  --num-redundant-experts 32 \
#  --eplb-window-size 1000 \
#  --eplb-step-interval 3000 \
##################
#  --api-server-count=8 \
##################
      env:
        - name: VLLM_TORCH_PROFILER_DIR
          value: "/code/traces"
        - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
          value: "1"
        - name: VLLM_REPO_URL
          value: "https://github.com/tlrmchlsmth/vllm.git"
        - name: VLLM_BRANCH
          value: "for_michael_k"
        - name: VLLM_USE_DEEP_GEMM
          value: "1"
        - name: VLLM_ALL2ALL_BACKEND
        #  value: "naive"
          value: "pplx"
        #  value: "deepep_high_throughput"
        #  value: "deepep_low_latency"
        # Needed for GDRCOPY to be used.
        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        - name: NVSHMEM_DEBUG
          value: "INFO"
        # Uncomment for debugging
        #- name: NVSHMEM_DEBUG_SUBSYS
        #  value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "DEBUG"
        #MK - name: HF_HUB_CACHE
        #MK   value: /huggingface-cache
        - name: GH_TOKEN_FROM_SECRET
          valueFrom:
            secretKeyRef:
              name: gh-token-secret
              key: GH_TOKEN
              optional: true
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "6555"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP

      securityContext:
        capabilities:
          add:
          - "IPC_LOCK"
          - "SYS_RAWIO"
      resources:
        limits:
          nvidia.com/gpu: 1
          memory: 512Gi
          ephemeral-storage: 64Gi
          rdma/ib: 1
        requests:
          cpu: 32
          memory: 512Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: 1
          rdma/ib: 1
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - name: init-scripts-volume
          mountPath: /init-scripts
        - name: vllm
          mountPath: /code
      mountModelVolume: true
    
# When endpointPicker is present, create the endpoint picker (inference scheduler) objects:
# ServiceAccount, Role, Rolebinding, Deployment, Service
# When endpointPicker is not present, these objects are not created.
endpointPicker:
  image: ghcr.io/llm-d/llm-d-inference-scheduler:0.0.3
  debugLevel: 5
  service:
    type: ClusterIP
    port: 9002
    targetPort: 9002
    appProtocol: http2
  # To use a different role from the default, provide the name of the role
  # permissions: pod-read

  autoscaling:
    enabled: false
  replicas: 1
