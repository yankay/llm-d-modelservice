# generated by generate-example-output.sh
---
# Source: llm-d-modelservice/templates/epp-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pd-llm-d-modelservice-epp
  labels:
    helm.sh/chart: llm-d-modelservice-v0.2.7
    app.kubernetes.io/version: "v0.2.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pd-llm-d-modelservice
  labels:
    helm.sh/chart: llm-d-modelservice-v0.2.7
    app.kubernetes.io/version: "v0.2.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/epp-plugin-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: pd-llm-d-modelservice-epp
  namespace: default
data:
  default-config.yaml: |
    apiVersion: inference.networking.x-k8s.io/v1alpha1
    kind: EndpointPickerConfig
    plugins:
    - type: prefix-cache-scorer
      parameters:
        hashBlockSize: 5
        maxPrefixBlocksToMatch: 256
        lruCapacityPerServer: 31250
    - type: decode-filter
    - type: max-score-picker
    - type: single-profile-handler
    schedulingProfiles:
    - name: default
      plugins:
      - pluginRef: decode-filter
      - pluginRef: max-score-picker
      - pluginRef: prefix-cache-scorer
        weight: 50
  prefix-cache-tracking-config.yaml: |
    apiVersion: inference.networking.x-k8s.io/v1alpha1
    kind: EndpointPickerConfig
    plugins:
      - type: single-profile-handler
      - type: decode-filter
      - type: prefix-cache-scorer
        parameters:
          mode: cache_tracking
          indexerConfig:
            tokenProcessorConfig:
              blockSize: 64                         # must match vLLM block size if not default (16)
              hashSeed: "42"                        # must match PYTHONHASHSEED in vLLM pods
            kvBlockIndexConfig:
              enableMetrics: true                   # enable kv-block index metrics (prometheus)
              metricsLoggingInterval: 60000000000   # log kv-block metrics as well (1m in nanoseconds)
      - type: kv-cache-scorer # kv-cache-utilization
      - type: queue-scorer
      - type: max-score-picker
    schedulingProfiles:
      - name: default
        plugins:
          - pluginRef: decode-filter
          - pluginRef: prefix-cache-scorer
            weight: 3.0
          - pluginRef: kv-cache-scorer
            weight: 1.0
          - pluginRef: queue-scorer
            weight: 1.0
          - pluginRef: max-score-picker
  prefix-estimate-config.yaml: |
    apiVersion: inference.networking.x-k8s.io/v1alpha1
    kind: EndpointPickerConfig
    plugins:
    - type: single-profile-handler
    - type: decode-filter
    - type: prefix-cache-scorer
    - type: load-aware-scorer
    - type: max-score-picker
    schedulingProfiles:
    - name: default
      plugins:
      - pluginRef: decode-filter
      - pluginRef: prefix-cache-scorer
        weight: 2.0
      - pluginRef: load-aware-scorer
        weight: 1.0
      - pluginRef: max-score-picker
  default-pd-config.yaml: |
    apiVersion: inference.networking.x-k8s.io/v1alpha1
    kind: EndpointPickerConfig
    plugins:
    - type: prefill-header-handler
    - type: prefix-cache-scorer
      parameters:
        hashBlockSize: 5
        maxPrefixBlocksToMatch: 256
        lruCapacityPerServer: 31250
    - type: prefill-filter
    - type: decode-filter
    - type: max-score-picker
    - type: pd-profile-handler
      parameters:
        threshold: 10
        hashBlockSize: 5
    schedulingProfiles:
    - name: prefill
      plugins:
      - pluginRef: prefill-filter
      - pluginRef: max-score-picker
      - pluginRef: prefix-cache-scorer
        weight: 50
    - name: decode
      plugins:
      - pluginRef: decode-filter
      - pluginRef: max-score-picker
      - pluginRef: prefix-cache-scorer
        weight: 50
---
# Source: llm-d-modelservice/templates/epp-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pd-llm-d-modelservice-epp
rules:
- apiGroups:
  - inference.networking.x-k8s.io
  resources:
  - inferencemodels
  - inferencepools
  verbs:
  - get
  - watch
  - list
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - watch
  - list
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - get
  - watch
  - list
- apiGroups:
  - authentication.k8s.io
  resources:
  - tokenreviews
  verbs:
  - create
- apiGroups:
  - authorization.k8s.io
  resources:
  - subjectaccessreviews
  verbs:
  - create
---
# Source: llm-d-modelservice/templates/epp-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pd-llm-d-modelservice-epp
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pd-llm-d-modelservice-epp
subjects:
- kind: ServiceAccount
  name: pd-llm-d-modelservice-epp
---
# Source: llm-d-modelservice/templates/epp-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: pd-llm-d-modelservice-epp
  labels:
    helm.sh/chart: llm-d-modelservice-v0.2.7
    app.kubernetes.io/version: "v0.2.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc-ext-proc
      port: 9002
      targetPort: 9002
      protocol: TCP
      appProtocol: http2
  selector:
    llm-d.ai/epp: pd-llm-d-modelservice-epp
---
# Source: llm-d-modelservice/templates/decode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pd-llm-d-modelservice-decode
  labels:
    helm.sh/chart: llm-d-modelservice-v0.2.7
    app.kubernetes.io/version: "v0.2.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: pd-llm-d-modelservice
      llm-d.ai/role: decode
  template:
    metadata:
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: pd-llm-d-modelservice
        llm-d.ai/role: decode
    spec:
      initContainers:
        - name: routing-proxy
          args:
            - --port=8000
            - --vllm-port=8200
            - --connector=nixlv2
            - -v=5
            - --secure-proxy=false
          image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.2.0
          imagePullPolicy: Always
          ports:
            - containerPort: 8000
          resources: {}
          restartPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
    
      serviceAccountName: pd-llm-d-modelservice
      volumes:
        - emptyDir: {}
          name: metrics-volume
        - name: model-storage
          emptyDir:
            sizeLimit: 20Gi
        
      containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d:v0.2.0
          
          command: ["vllm", "serve"]
          args:
            - "facebook/opt-125m"
            - --port
            - "8200"
            - --served-model-name
            - "facebook/opt-125m"
            
            - --enforce-eager
            - --kv-transfer-config
            - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
          env:
          - name: CUDA_VISIBLE_DEVICES
            value: "0"
          - name: UCX_TLS
            value: cuda_ipc,cuda_copy,tcp
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "5557"
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: DP_SIZE
            value: "1"
          - name: TP_SIZE
            value: "1"
          
          - name: HF_HOME
            value: /model-cache
          ports:
          - containerPort: 8200
            protocol: TCP
          - containerPort: 5557
            protocol: TCP
          
          resources:
            limits:
              cpu: "16"
              memory: 16Gi
              nvidia.com/gpu: "1"
            requests:
              cpu: "16"
              memory: 16Gi
              nvidia.com/gpu: "1"
          
          volumeMounts:
            - name: model-storage
              mountPath: /model-cache
---
# Source: llm-d-modelservice/templates/epp-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pd-llm-d-modelservice-epp
  labels:
    llm-d.ai/epp: pd-llm-d-modelservice-epp
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/epp: pd-llm-d-modelservice-epp
  template:
    metadata:
      labels:
        llm-d.ai/epp: pd-llm-d-modelservice-epp
    spec:
      containers:
      - name: epp
        imagePullPolicy: Always
        image: ghcr.io/llm-d/llm-d-inference-scheduler:v0.2.1
        args:
        - --poolName
        - pd-llm-d-modelservice
        - --poolNamespace
        - default
        - -v
        - "4"
        - --zap-encoder
        - json
        - --grpcPort
        - "9002"
        - --grpcHealthPort
        - "9003"
        - "-configFile"
        - "config/default-config.yaml"
        ports:
        - containerPort: 9002
          name: grpc
          protocol: TCP
        - containerPort: 9003
          name: grpc-health
          protocol: TCP
        - containerPort: 9090
          name: metrics
          protocol: TCP
        readinessProbe:
          grpc:
            port: 9003
            service: envoy.service.ext_proc.v3.ExternalProcessor
          initialDelaySeconds: 5
          timeoutSeconds: 1
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        livenessProbe:
          grpc:
            port: 9003
            service: envoy.service.ext_proc.v3.ExternalProcessor
          initialDelaySeconds: 5
          timeoutSeconds: 1
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        volumeMounts:
          - name: plugins-config-volume
            mountPath: "/config"
      volumes:
      - name: plugins-config-volume
        configMap:
          name: pd-llm-d-modelservice-epp
      serviceAccount: pd-llm-d-modelservice-epp
      serviceAccountName: pd-llm-d-modelservice-epp
---
# Source: llm-d-modelservice/templates/prefill-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pd-llm-d-modelservice-prefill
  labels:
    helm.sh/chart: llm-d-modelservice-v0.2.7
    app.kubernetes.io/version: "v0.2.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: pd-llm-d-modelservice
      llm-d.ai/role: prefill
  template:
    metadata:
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: pd-llm-d-modelservice
        llm-d.ai/role: prefill
    spec:
    
      serviceAccountName: pd-llm-d-modelservice
      volumes:
        - emptyDir: {}
          name: metrics-volume
        - name: model-storage
          emptyDir:
            sizeLimit: 20Gi
        
      containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d:v0.2.0
          
          command: ["vllm", "serve"]
          args:
            - "facebook/opt-125m"
            - --port
            - "8000"
            - --served-model-name
            - "facebook/opt-125m"
            
            - --enforce-eager
            - --kv-transfer-config
            - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
          env:
          - name: CUDA_VISIBLE_DEVICES
            value: "0"
          - name: UCX_TLS
            value: cuda_ipc,cuda_copy,tcp
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "5557"
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: DP_SIZE
            value: "1"
          - name: TP_SIZE
            value: "1"
          
          - name: HF_HOME
            value: /model-cache
          ports:
          - containerPort: 8000
            protocol: TCP
          - containerPort: 5557
            protocol: TCP
          
          resources:
            limits:
              cpu: "16"
              memory: 16Gi
              nvidia.com/gpu: "1"
            requests:
              cpu: "16"
              memory: 16Gi
              nvidia.com/gpu: "1"
          
          volumeMounts:
            - name: model-storage
              mountPath: /model-cache
---
# Source: llm-d-modelservice/templates/inferencemodel.yaml
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferenceModel
metadata:
  labels:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: pd-llm-d-modelservice
  name: pd-llm-d-modelservice
spec:
  criticality: Critical
  modelName: facebook/opt-125m
  poolRef:
    name: pd-llm-d-modelservice
---
# Source: llm-d-modelservice/templates/inferencepool.yaml
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferencePool
metadata:
  name: pd-llm-d-modelservice
  namespace: default
spec:
  extensionRef:
    failureMode: FailClose
    group: ""
    kind: Service
    name: pd-llm-d-modelservice-epp
  selector:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: pd-llm-d-modelservice
  targetPortNumber: 8000
---
# Source: llm-d-modelservice/templates/httproute.yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: pd-llm-d-modelservice
  namespace: default
  labels:
    helm.sh/chart: llm-d-modelservice-v0.2.7
    app.kubernetes.io/version: "v0.2.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-install,post-upgrade
spec:
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: inference-gateway
    namespace: 'default'
  rules:
    - backendRefs:
      - group: inference.networking.x-k8s.io
        kind: InferencePool
        name: pd-llm-d-modelservice
        port: 8000
        weight: 1
      timeouts:
        backendRequest: 0s
        request: 0s
      matches:
      - headers:
        - name: x-model-name
          type: Exact
          value: 'facebook/opt-125m'
