multinode: false  # If true, creates LWS instead of deployments  
inferencePool: true 
inferenceModel: true 
httpRoute: true 

routing: 
  # This is the model name for the OpenAI request
  modelName: facebook/opt-125m
  ports:
    servicePort: 8000   # Sidecar listens on this port for requests. If there's no sidecar, the request goes here
    internalPort: 8200  # Sidecar forwards request to vllm container on this port 
    proxy:
      targetPort: 8000
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: inference-gateway-kgateway

modelArtifacts:
  # When specfying the URI with `hf` prefix, the <repo-id>/<model-id> string
  # is extracted and exposed as a template variable that can be used as {{ .HFModelName }}
  prefix: "oci"
  artifact: facebook/opt-125m
  authSecretName: "hf-secret"
  size: 5Mi 
  imagePullPolicy: IfNotPresent

# describe decode pods
decode:
  enableService: false
  replicas: 1
  
  # for LWS
  parallelism:  
    tensor: 8
    data: 1
    dataLocal: 1 
  
  acceleratorTypes:
    labelKey: nvidia.com/gpu.product
    labelValues:
      # According to the blog, Scout requires H100s
      - NVIDIA-H100
  # initContainers:
  containers:
  - name: "vllm"
    image: "vllm-ai/vllm:latest"  
    args:
      - "HFModelName"
    env:
    - name: "VLLM_LOG_LEVEL"
      value: "DEBUG"  # Set to DEBUG for more detailed logs, or INFO for less verbose logs
    envFrom:
      - configMapRef:
          name: vllm-config
    resources:
      requests:
        cpu: "1"          # Request 1 CPU core
        memory: "4Gi"    # Request 4 GiB of memory
      limits:
        cpu: "2"          # Limit to 2 CPU cores
        memory: "8Gi"     # Limit to 8 GiB of memory
    mountModelVolume: true 

# describe the prefill pods (looks the same as above)
prefill:
  replicas: 1
  containers:
    - name: "vllm"
      image: "vllm-ai/vllm:latest"  
      args:
        - "HFModelName"
      env: 
        - name: ok 
          value: ok 
      mountModelVolume: true
    - name: "v2"
      image: "vllm-ai/vllm:latest"  
      volumeMounts: 
        - name: whatever 
          mountPath: something 
  volumes: 
    - name: ok 
      emptyDir:
        sizeLimit: 5Gi 
    - name: ok2
      emptyDir:
        sizeLimit: 5Gi 
    
endpointPicker:
  # This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
  service:
    # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
    type: ClusterIP
    # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
    port: 9002
    targetPort: 9002
    appProtocol: http2
