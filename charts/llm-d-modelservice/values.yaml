# vllm-sim values

# This values.yaml file creates the resources for random

multinode: false          # If true, creates LWS instead of deployments
inferencePool: true
httpRoute: true

routing:
  # This is the model name for the OpenAI request
  modelName: random
  servicePort: 8000   # Sidecar listens on this port for requests. If there's no sidecar, the request goes here
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
    targetPort: 8200
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: llm-d-inference-gateway

modelArtifacts:
  uri: "hf://random/modelid"
  size: 5Mi

# describe decode pods
decode:
  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:0.0.4"
    args:
      - "--model"
      - "random"
      - "--port"
      - "8200"  # targetPort
    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true
prefill:
  replicas: 1
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:0.0.4"
    args:
      - "--model"
      - "random"
      - "--port"
      - "8000"  # servicePort
    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true

endpointPicker:
  # This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
  service:
    # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
    type: ClusterIP
    # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
    port: 9002
    targetPort: 9002
    appProtocol: http2
  debugLevel: 6
  disableReadinessProbe: true
  disableLivenessProbe: true
  replicas: 1
  permissions: pod-read
