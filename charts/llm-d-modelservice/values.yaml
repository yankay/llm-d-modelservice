# This values.yaml file is a default values file. It can be overridden using another.

# schedulerName -- Name of the scheduler to use for scheduling model pods
# schedulerName: default-scheduler

modelArtifacts:
  # model URI. One of:
  #   hf://model/name - model on Hugging Face
  #   pvc://pvc_name/path/to/model - model on existing persistant storage
  #   oci:// not yet supported
  uri: "hf://random/modelid"
  # size of volume to create to hold the model
  size: 5Mi
  # name of secret containing credentials for accessing the model (e.g., HF_TOKEN)
  # authSecretName:

# When true, a LeaderWorkerSet is used instead of a Deployment
multinode: false

# Describe routing requirements. In addition to service level routing (OpenAI model name, service port)
# also describes elements for Gateway API Inference Extension configuration
routing:
  # This is the model name for OpenAI requests
  modelName: random/modelId
  # port the VLLM services listen on
  # when a sidecar (proxy) is used it will listen on this port and forward to VLLM on proxy.targetPort
  servicePort: 8000

  # Configuration of VLLM routing sidecar
  # cf. https://github.com/llm-d/llm-d-routing-sidecar/
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.2.0
    # target port on which VLLM should listen
    targetPort: 8200
    # Specify a conenctor to overwrite the default: `nixl`
    connector: nixlv2

    # Boolean: adds the `--secure-proxy` flag to the routingSidecar with your chosen value.  Arg is ommitted by default for compatability with legacy sidecar images.
    # secure: true

    # Boolean: whether to use TLS when sending requests to prefillers. Arg is ommitted by default for compatability with legacy sidecar images.
    # prefillerUseTLS: true

    # The path to the certificate for secure proxy.  Arg is ommitted by default for compatability with legacy sidecar images.
    # certPath: "/certs"

    # Overwrite the verbosity of logging in the sidecar (defaults to 5)
    # debugLevel: 5


  # Reference to parent gateway
  # cf. https://gateway-api.sigs.k8s.io/api-types/gateway/
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: inference-gateway

  # Configuration of InferencePool
  # cf. https://gateway-api-inference-extension.sigs.k8s.io/reference/spec/#inferencepool
  inferencePool:
    create: true
    # name: OVERRIDE_NAME
    # to use a different epp service than the one created when routing.epp.create: true
    # extensionRef:

  inferenceModel:
    create: false
    # Criticality options: ["Critical", "Standard", "Sheddable"], see: https://github.com/kubernetes-sigs/gateway-api-inference-extension/blob/main/config/crd/bases/inference.networking.x-k8s.io_inferencemodels.yaml#L70-L84
    criticality: Critical

  # Configuration of HTTPRoute (mapping of requests through gateway to InferencePool)
  # cf. https://gateway-api.sigs.k8s.io/api-types/httproute/
  httpRoute:
    create: true
    # when specifiying rules it will overwrite the entire rules block (matches included)
    # rules:
    #   - backendRefs:
    #       - group: inference.networking.x-k8s.io
    #         kind: InferencePool
    #         name: inference-pool-name
    #         port: 8000
    #         weight: 1
    #     matches:
    #       - path:
    #           type: PathPrefix
    #           value: /
    # when specifiying matches and not rules, it will use the default backendRef block but overwrite just the matches section of a single rule
    matches:
      - path:
          type: PathPrefix
          value: /
        # example over-riding matches
        # headers:
        # - name: x-model-name
        #   type: Exact
        #   value: facebook/opt-125m

  # Configuration of EPP (endpoint picker)
  # cf. https://github.com/llm-d/llm-d-inference-scheduler
  epp:
    create: true
    service:
      type: ClusterIP
      port: 9002
      targetPort: 9002
      appProtocol: http2
    image: ghcr.io/llm-d/llm-d-inference-scheduler:v0.2.0
    replicas: 1
    debugLevel: 4
    disableReadinessProbe: false
    disableLivenessProbe: false
    # To override the name of the inferencepool
    # inferencePool:
    # -- Default environment variables for endpoint picker, use `defaultEnvVarsOverride` to override default behavior by defining the same variable again.
    # Ref: https://github.com/llm-d/llm-d-inference-scheduler/blob/main/docs/architecture.md#scorers--configuration

    # The name of the plugin file to use. Some default files are provided to you: default-config.yaml, prefix-cache-tracking-config.yaml,
    # prefix-estimate-config.yaml, default-pd-config.yaml, or you may define a custom config below and select it with the pluginsConfigFile field.
    pluginsConfigFile: "default-config.yaml"

    # Adding a custom plugin config via the pluginsCustomConfig field. Inside there should be an entry to a confimap of file containing the `EndpointPickerConfig`
    # pluginsCustomConfig:
    #   custom-plugins.yaml: |
    #     apiVersion: inference.networking.x-k8s.io/v1alpha1
    #     kind: EndpointPickerConfig
    #     plugins:
    #     - type: custom-scorer
    #       parameters:
    #         custom-threshold: 64
    #     - type: max-score-picker
    #     - type: single-profile-handler
    #     schedulingProfiles:
    #     - name: default
    #       plugins:
    #       - pluginRef: custom-scorer
    #         weight: 1
    #       - pluginRef: max-score-picker
    #         weight: 1


    env:
      # Include any Environment variables to epp here, or configure scorers for a legacy epp image, ex:
    # - name: ENABLE_KVCACHE_AWARE_SCORER
    #   value: "false"


# Decode pod configuration
decode:
  create: true
  autoscaling:
    enabled: false

  replicas: 1
  # schedulerName -- Name of the scheduler to use for scheduling decode pods (overrides global schedulerName)
  # schedulerName: decode-scheduler
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:v0.3.0"
    modelCommand: imageDefault
    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true
    volumeMounts:
    - name: metrics-volume
      mountPath: /.config
  volumes:
  - name: metrics-volume
    emptyDir: {}

  # hostIPC -- Boolean: Use the host's ipc namespace.
  # -- Not set by default.
  # -- Only an option for LWS (multinode) See: https://github.com/kubernetes-sigs/lws/blob/main/config/crd/bases/leaderworkerset.x-k8s.io_leaderworkersets.yaml#L12196.
  # hostIPC: false

  # hostPID -- Boolean: Use the host's pid namespace.
  # -- Not set by default.
  # -- Only an option for LWS (multinode) See: https://github.com/kubernetes-sigs/lws/blob/main/config/crd/bases/leaderworkerset.x-k8s.io_leaderworkersets.yaml#L12207.
  # hostPID: false

  # subGroupPolicy -- object: SubGroupPolicy describes the policy that will be applied when creating subgroups in each replica.
  # -- Not set by default
  # -- Only an option for LWS (multinode) See: https://github.com/kubernetes-sigs/lws/blob/main/config/crd/bases/leaderworkerset.x-k8s.io_leaderworkersets.yaml#L8207
  # subGroupPolicy:
  #   subGroupSize: 8

  # subGroupExclusiveToplogy -- Boolean: Should the `subgroup-exclusive-topology` annotation be added to the LWS
  # -- Not set by default
  # -- Only an option for LWS (multinode)
  # subGroupExclusiveToplogy: true

# Prefill pod configuation
prefill:
  create: true
  autoscaling:
    enabled: false

  replicas: 0
  # schedulerName -- Name of the scheduler to use for scheduling prefill pods (overrides global schedulerName)
  # schedulerName: prefill-scheduler
  containers:
  - name: "vllm"
    image: "ghcr.io/llm-d/llm-d-inference-sim:v0.3.0"
    modelCommand: imageDefault
    ports:
      - containerPort: 5557
        protocol: TCP
    mountModelVolume: true
    volumeMounts:
    - name: metrics-volume
      mountPath: /.config
  volumes:
  - name: metrics-volume
    emptyDir: {}

  # hostIPC -- Boolean: Use the host's ipc namespace.
  # -- Not set by default.
  # -- Only an option for LWS (multinode) See: https://github.com/kubernetes-sigs/lws/blob/main/config/crd/bases/leaderworkerset.x-k8s.io_leaderworkersets.yaml#L12196.
  # hostIPC: false

  # hostPID -- Boolean: Use the host's pid namespace.
  # -- Not set by default.
  # -- Only an option for LWS (multinode) See: https://github.com/kubernetes-sigs/lws/blob/main/config/crd/bases/leaderworkerset.x-k8s.io_leaderworkersets.yaml#L12207.
  # hostPID: false

  # subGroupPolicy -- object: SubGroupPolicy describes the policy that will be applied when creating subgroups in each replica.
  # -- Not set by default
  # -- Only an option for LWS (multinode) See: https://github.com/kubernetes-sigs/lws/blob/main/config/crd/bases/leaderworkerset.x-k8s.io_leaderworkersets.yaml#L8207
  # subGroupPolicy:
  #   subGroupSize: 8

  # subGroupExclusiveToplogy -- Boolean: Should the `subgroup-exclusive-topology` annotation be added to the LWS
  # -- Not set by default
  # -- Only an option for LWS (multinode)
  # subGroupExclusiveToplogy: true
